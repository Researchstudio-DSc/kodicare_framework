{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Comparison Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24178/1526167184.py:11: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f_comp_path, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "comp_data_dir = \"../../data/trec_covid_topic_modelling\"\n",
    "f_comp_path = os.path.join(comp_data_dir, \"abcnews-date-text.csv\")\n",
    "f_comp_out_path = os.path.join(comp_data_dir, \"abcnews-date-text.csv.tokenized_quick.txt\")\n",
    "\n",
    "data = pd.read_csv(f_comp_path, error_bad_lines=False)\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "def preprocess_simple(texts):\n",
    "    results = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        result = []\n",
    "        results.append(result)\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct and len(token.text) > 3:\n",
    "                if token.like_num:\n",
    "                    result.append(\"<NUM>\")\n",
    "                else:\n",
    "                    result.append(token.lemma_.lower())\n",
    "    return results\n",
    "\n",
    "clean_p = re.compile(r'[^\\w\\s]+|\\d+', re.UNICODE)\n",
    "\n",
    "def preprocess_quick(texts):\n",
    "    results = []\n",
    "    for doc in texts:\n",
    "        clean_string = clean_p.sub(' ', doc)\n",
    "        results.append(clean_string.lower().split())\n",
    "    return results\n",
    "\n",
    "#processed_docs_news = preprocess_simple(documents['headline_text'])\n",
    "processed_docs_news = preprocess_quick(documents['headline_text'])\n",
    "\n",
    "with open(f_comp_out_path, \"w\") as fp:\n",
    "    for idx, doc in enumerate(processed_docs_news):\n",
    "        doc_text = \" \".join(doc)\n",
    "        #fp.write(f'{idx},\"{doc_text}\"\\n')\n",
    "        fp.write(f'{doc_text}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DTC Evolving Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29488\n",
      "29488\n",
      "29488\n",
      "26540\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "data_dir = \"/home/tfink/data/kodicare/trec-covid/\"\n",
    "f_dtc_uid_path = os.path.join(data_dir, \"TREC-COVID_dtc_evolving_corduids.pkl\")\n",
    "f_trec_covid_path = os.path.join(data_dir, \"TREC-COVID_complete_content.csv.tokenized_lm.txt\")\n",
    "f_trec_covid_abstracts_path = os.path.join(data_dir, \"TREC-COVID_complete_content_abstracts.csv.tokenized_lm.txt\")\n",
    "f_out_path = os.path.join(data_dir, \"dtc_evolving_lm\")\n",
    "f_abstracts_out_path = os.path.join(data_dir, \"dtc_evolving_abstracts_lm\")\n",
    "\n",
    "skip_overlap = False\n",
    "\n",
    "with open(f_dtc_uid_path, \"rb\") as fp:\n",
    "    split_datasets = pickle.load(fp)\n",
    "\n",
    "\n",
    "s0 = set(split_datasets[0])\n",
    "s1 = set(split_datasets[1])\n",
    "s_x = set(split_datasets[10])\n",
    "print(len(s0))\n",
    "print(len(s1))\n",
    "print(len(s_x))\n",
    "print(len(s0 & s1))\n",
    "print(len(s0 & s_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path):\n",
    "    documents = {}\n",
    "    with open(path, \"r\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "        for line in reader:\n",
    "            cord_uid, doc_text = line\n",
    "            if cord_uid in documents:\n",
    "                if len(documents[cord_uid]) > len(doc_text):\n",
    "                    continue\n",
    "            documents[cord_uid] = doc_text\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_document_sentences(path):\n",
    "    documents = {}\n",
    "    with open(path, \"r\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "        for line in reader:\n",
    "            cord_uid, doc_text = line\n",
    "            if cord_uid not in documents:\n",
    "                documents[cord_uid] = []\n",
    "            documents[cord_uid].append(doc_text)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def save_split_documents(out_path, documents, split_datasets):\n",
    "    last_ids = set()\n",
    "\n",
    "    # for now, only save datasets that do not overlap at all\n",
    "    for idx, ds in enumerate(split_datasets):\n",
    "        if len(last_ids & set(ds)) > 0:\n",
    "            print(f\"Skipping ds {idx}\")\n",
    "            continue\n",
    "        print(f\"Saving ds {idx}\")\n",
    "        last_ids = set(ds)\n",
    "        with open(os.path.join(out_path, f\"{idx}.csv\"), \"w\") as fp:\n",
    "            for cord_uid in ds:\n",
    "                doc_text = documents[cord_uid]\n",
    "                fp.write(f'{cord_uid},\"{doc_text}\"\\n')\n",
    "\n",
    "\n",
    "def save_split_documents_lm(out_path, documents, split_datasets, skip_overlap=True):\n",
    "    last_ids = set()\n",
    "\n",
    "    # for now, only save datasets that do not overlap at all\n",
    "    for idx, ds in enumerate(split_datasets):\n",
    "        if skip_overlap and len(last_ids & set(ds)) > 0:\n",
    "            print(f\"Skipping ds {idx}\")\n",
    "            continue\n",
    "        print(f\"Saving ds {idx}\")\n",
    "        last_ids = set(ds)\n",
    "        with open(os.path.join(out_path, f\"{idx}.txt\"), \"w\") as fp:\n",
    "            for cord_uid in ds:\n",
    "                sentences = documents[cord_uid]\n",
    "                for sent in sentences:\n",
    "                    fp.write(f'{sent}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ds 0\n",
      "Skipping ds 1\n",
      "Skipping ds 2\n",
      "Skipping ds 3\n",
      "Skipping ds 4\n",
      "Skipping ds 5\n",
      "Skipping ds 6\n",
      "Skipping ds 7\n",
      "Skipping ds 8\n",
      "Skipping ds 9\n",
      "Skipping ds 10\n",
      "Saving ds 11\n",
      "Skipping ds 12\n",
      "Skipping ds 13\n",
      "Skipping ds 14\n",
      "Skipping ds 15\n",
      "Skipping ds 16\n",
      "Skipping ds 17\n",
      "Skipping ds 18\n",
      "Skipping ds 19\n",
      "Skipping ds 20\n",
      "Skipping ds 21\n",
      "Saving ds 22\n",
      "Skipping ds 23\n",
      "Skipping ds 24\n",
      "Skipping ds 25\n",
      "Skipping ds 26\n",
      "Skipping ds 27\n",
      "Skipping ds 28\n",
      "Skipping ds 29\n",
      "Skipping ds 30\n",
      "Skipping ds 31\n",
      "Skipping ds 32\n",
      "Saving ds 33\n",
      "Skipping ds 34\n",
      "Skipping ds 35\n",
      "Skipping ds 36\n",
      "Skipping ds 37\n",
      "Skipping ds 38\n",
      "Skipping ds 39\n",
      "Skipping ds 40\n"
     ]
    }
   ],
   "source": [
    "documents = load_document_sentences(f_trec_covid_path)\n",
    "#save_split_documents(out_path=f_out_path, documents=documents, split_datasets=split_datasets)\n",
    "save_split_documents_lm(out_path=f_out_path, documents=documents, split_datasets=split_datasets, skip_overlap=skip_overlap)\n",
    "documents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ds 0\n",
      "Saving ds 1\n",
      "Saving ds 2\n",
      "Saving ds 3\n",
      "Saving ds 4\n",
      "Saving ds 5\n",
      "Saving ds 6\n",
      "Saving ds 7\n",
      "Saving ds 8\n",
      "Saving ds 9\n",
      "Saving ds 10\n",
      "Saving ds 11\n",
      "Saving ds 12\n",
      "Saving ds 13\n",
      "Saving ds 14\n",
      "Saving ds 15\n",
      "Saving ds 16\n",
      "Saving ds 17\n",
      "Saving ds 18\n",
      "Saving ds 19\n",
      "Saving ds 20\n",
      "Saving ds 21\n",
      "Saving ds 22\n",
      "Saving ds 23\n",
      "Saving ds 24\n",
      "Saving ds 25\n",
      "Saving ds 26\n",
      "Saving ds 27\n",
      "Saving ds 28\n",
      "Saving ds 29\n",
      "Saving ds 30\n",
      "Saving ds 31\n",
      "Saving ds 32\n",
      "Saving ds 33\n",
      "Saving ds 34\n",
      "Saving ds 35\n",
      "Saving ds 36\n",
      "Saving ds 37\n",
      "Saving ds 38\n",
      "Saving ds 39\n",
      "Saving ds 40\n"
     ]
    }
   ],
   "source": [
    "documents = load_document_sentences(f_trec_covid_abstracts_path)\n",
    "#save_split_documents(out_path=f_abstracts_out_path, documents=documents, split_datasets=split_datasets)\n",
    "save_split_documents_lm(out_path=f_abstracts_out_path, documents=documents, split_datasets=split_datasets, skip_overlap=skip_overlap)\n",
    "documents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kodicare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

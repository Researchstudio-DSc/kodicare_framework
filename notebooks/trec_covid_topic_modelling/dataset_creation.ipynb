{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Comparison Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "comp_data_dir = \"../../data/trec_covid_topic_modelling\"\n",
    "f_comp_path = os.path.join(comp_data_dir, \"abcnews-date-text.csv\")\n",
    "f_comp_out_path = os.path.join(comp_data_dir, \"abcnews-date-text.csv.tokenized.txt\")\n",
    "\n",
    "data = pd.read_csv(f_comp_path, error_bad_lines=False)\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "def preprocess_simple(texts):\n",
    "    results = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        result = []\n",
    "        results.append(result)\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct and len(token.text) > 3:\n",
    "                if token.like_num:\n",
    "                    result.append(\"<NUM>\")\n",
    "                else:\n",
    "                    result.append(token.lemma_.lower())\n",
    "    return results\n",
    "\n",
    "processed_docs_news = preprocess_simple(documents['headline_text'])\n",
    "\n",
    "with open(f_comp_out_path, \"w\") as fp:\n",
    "    for idx, doc in enumerate(processed_docs_news):\n",
    "        doc_text = \" \".join(doc)\n",
    "        fp.write(f'{idx},\"{doc_text}\"\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DTC Evolving Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29488\n",
      "29488\n",
      "29488\n",
      "26540\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "data_dir = \"/home/tfink/data/kodicare/trec-covid/\"\n",
    "f_dtc_uid_path = os.path.join(data_dir, \"TREC-COVID_dtc_evolving_corduids.pkl\")\n",
    "f_trec_covid_path = os.path.join(data_dir, \"TREC-COVID_complete_content.csv.tokenized.txt\")\n",
    "f_out_path = os.path.join(data_dir, \"dtc_evolving\")\n",
    "\n",
    "with open(f_dtc_uid_path, \"rb\") as fp:\n",
    "    split_datasets = pickle.load(fp)\n",
    "\n",
    "\n",
    "s0 = set(split_datasets[0])\n",
    "s1 = set(split_datasets[1])\n",
    "s_x = set(split_datasets[10])\n",
    "print(len(s0))\n",
    "print(len(s1))\n",
    "print(len(s_x))\n",
    "print(len(s0 & s1))\n",
    "print(len(s0 & s_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "with open(f_trec_covid_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    for line in reader:\n",
    "        cord_uid, doc_text = line\n",
    "        if cord_uid in documents:\n",
    "            if len(documents[cord_uid]) > len(doc_text):\n",
    "                continue\n",
    "        documents[cord_uid] = doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ds 0\n",
      "Skipping ds 1\n",
      "Skipping ds 2\n",
      "Skipping ds 3\n",
      "Skipping ds 4\n",
      "Skipping ds 5\n",
      "Skipping ds 6\n",
      "Skipping ds 7\n",
      "Skipping ds 8\n",
      "Skipping ds 9\n",
      "Skipping ds 10\n",
      "Saving ds 11\n",
      "Skipping ds 12\n",
      "Skipping ds 13\n",
      "Skipping ds 14\n",
      "Skipping ds 15\n",
      "Skipping ds 16\n",
      "Skipping ds 17\n",
      "Skipping ds 18\n",
      "Skipping ds 19\n",
      "Skipping ds 20\n",
      "Skipping ds 21\n",
      "Saving ds 22\n",
      "Skipping ds 23\n",
      "Skipping ds 24\n",
      "Skipping ds 25\n",
      "Skipping ds 26\n",
      "Skipping ds 27\n",
      "Skipping ds 28\n",
      "Skipping ds 29\n",
      "Skipping ds 30\n",
      "Skipping ds 31\n",
      "Skipping ds 32\n",
      "Saving ds 33\n",
      "Skipping ds 34\n",
      "Skipping ds 35\n",
      "Skipping ds 36\n",
      "Skipping ds 37\n",
      "Skipping ds 38\n",
      "Skipping ds 39\n",
      "Skipping ds 40\n"
     ]
    }
   ],
   "source": [
    "last_ids = set()\n",
    "\n",
    "# for now, only save datasets that do not overlap at all\n",
    "for idx, ds in enumerate(split_datasets):\n",
    "    if len(last_ids & set(ds)) > 0:\n",
    "        print(f\"Skipping ds {idx}\")\n",
    "        continue\n",
    "    print(f\"Saving ds {idx}\")\n",
    "    last_ids = set(ds)\n",
    "    with open(os.path.join(f_out_path, f\"{idx}.csv\"), \"w\") as fp:\n",
    "        for cord_uid in ds:\n",
    "            doc_text = documents[cord_uid]\n",
    "            fp.write(f'{cord_uid},\"{doc_text}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kodicare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

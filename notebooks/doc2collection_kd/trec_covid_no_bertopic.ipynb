{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "random_state = 42\n",
    "#random_state = 420\n",
    "random.seed(random_state)\n",
    "\n",
    "f_tokenized_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/0.csv\"\n",
    "f_tokenized_other_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/11.csv\"\n",
    "f_tokenized_contrast_path = \"/home/tfink/projects/rsa/kodicare/kodicare_framework/data/trec_covid_topic_modelling/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cleaned(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "        documents = {}\n",
    "        document_uids = []\n",
    "        for line in tqdm(reader):\n",
    "            cord_uid, passage_text_cleaned = line\n",
    "            if cord_uid not in documents:\n",
    "                documents[cord_uid] = []\n",
    "                document_uids.append(cord_uid)\n",
    "            documents[cord_uid].append(passage_text_cleaned)\n",
    "        return documents, document_uids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115954it [00:08, 131322.47it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of this assumption removes the premium on rarity and also makes it possible to find evidence of affinity between two species which occur in most of the samples taken. it seems neither more nor less arbitrary than the use of the total number of samples for the latter is usually determined arbitrarily by the person doing the work, often mainly on grounds of practicality or subjective judgment that the number taken is sufficient.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[document_uids[40]][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic TF-IDF delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29487, 619934)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc_iter(doc_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        yield \" \".join(doc_dict[uid])\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.75)\n",
    "X_train = count_vect.fit_transform(doc_iter(doc_dict=documents, document_uids=document_uids))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29487, 619934)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:50<00:00, 13.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29487, 29487)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_cosine_similarity(X,Y, batch_size=4096, dtype=np.float64):\n",
    "    X_size = X.shape[0]\n",
    "    Y_size = Y.shape[0]\n",
    "    steps = int(np.ceil(Y_size / batch_size))\n",
    "    sims = np.zeros((X_size, Y_size), dtype=dtype)\n",
    "    for i in tqdm(range(steps)):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        sim = cosine_similarity(X,Y[start_idx:end_idx,:]).astype(dtype=dtype)\n",
    "        sims[:,start_idx:end_idx] = sim\n",
    "    return sims\n",
    "    \n",
    "sims = batched_cosine_similarity(X_train,X_train, batch_size=4096, dtype=np.float16)\n",
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8555, 0.6084, 0.6025, 0.5713, 0.52, 0.26, 0.2401, 0.2164, 0.2086]\n",
      "[1.0, 0.542, 0.155, 0.1259, 0.1249, 0.1241, 0.11456, 0.1034, 0.0872, 0.08295]\n",
      "[1.0, 0.4958, 0.317, 0.3013, 0.2129, 0.2103, 0.2083, 0.2013, 0.1964, 0.194]\n",
      "[1.0, 0.156, 0.142, 0.11176, 0.0987, 0.09076, 0.0879, 0.0871, 0.07935, 0.0787]\n",
      "[1.0, 0.3643, 0.2974, 0.2422, 0.2316, 0.2303, 0.227, 0.219, 0.2108, 0.2072]\n",
      "[1.0, 0.603, 0.5083, 0.352, 0.3372, 0.3372, 0.3372, 0.293, 0.293, 0.2515]\n",
      "[1.0, 0.3115, 0.2773, 0.249, 0.2422, 0.2386, 0.223, 0.2202, 0.2198, 0.2186]\n",
      "[1.0, 1.0, 0.85, 0.8413, 0.685, 0.618, 0.5996, 0.5723, 0.5527, 0.5474]\n",
      "[1.0, 0.735, 0.5845, 0.5845, 0.5693, 0.56, 0.5557, 0.533, 0.514, 0.514]\n",
      "[1.0, 0.09625, 0.09216, 0.082, 0.06012, 0.05707, 0.05624, 0.04724, 0.04376, 0.03833]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sorted(sims[:,i], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 1.0000, Top3: 0.6450, Top10: 0.4514\n"
     ]
    }
   ],
   "source": [
    "def get_mean_similarity(sim_X_Y, top_k):\n",
    "    # https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "    Y_len = sim_X_Y.shape[1]\n",
    "    mean_sims = []\n",
    "    for i in range(Y_len):\n",
    "        #top_y_sims = sorted(sims[:,i], reverse=True)[:top_k]\n",
    "        top_y_sims_idx = np.argpartition(sims[:,i], -top_k)[-top_k:]\n",
    "        top_y_sims = sims[:,i][top_y_sims_idx]\n",
    "        mean_sims.append(np.mean(top_y_sims))\n",
    "    return np.mean(mean_sims)\n",
    "    \n",
    "\n",
    "top_1 = get_mean_similarity(sims, top_k=1)\n",
    "top_3 = get_mean_similarity(sims, top_k=3)\n",
    "top_10 = get_mean_similarity(sims, top_k=10)\n",
    "\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1102137it [00:08, 132549.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29488, 619934)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_other_path)\n",
    "X_test = count_vect.transform(doc_iter(doc_dict=documents, document_uids=document_uids))\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:54<00:00, 14.27s/it]\n"
     ]
    }
   ],
   "source": [
    "sims = batched_cosine_similarity(X_train,X_test, batch_size=4096, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.4280, Top3: 0.3877, Top10: 0.3325\n"
     ]
    }
   ],
   "source": [
    "top_1 = get_mean_similarity(sims, top_k=1)\n",
    "top_3 = get_mean_similarity(sims, top_k=3)\n",
    "top_10 = get_mean_similarity(sims, top_k=10)\n",
    "\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast Dataset Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:00, 1333930.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    documents = []\n",
    "    document_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        documents.append(passage_text_cleaned)\n",
    "        document_uids.append(cord_uid)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 619934)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = count_vect.transform(documents)\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "sims = batched_cosine_similarity(X_train,X_test[:30000,:], batch_size=4096, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.2443, Top3: 0.2010, Top10: 0.1459\n"
     ]
    }
   ],
   "source": [
    "top_1 = get_mean_similarity(sims, top_k=1)\n",
    "top_3 = get_mean_similarity(sims, top_k=3)\n",
    "top_10 = get_mean_similarity(sims, top_k=10)\n",
    "\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Vocab based delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect2 = CountVectorizer(max_df=1.0, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "x = count_vect2.fit_transform([\"1 2 3 4 5 6 7 8 9 10\", \"1 2 3\"])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect2.vocabulary_.get('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [03:03<00:00, 17.03it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(embedding_model)\n",
    "passages_encoded = model.encode(passages[:100000], show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0, n_components=5, random_state=42, verbose=True)\n",
      "Wed Aug  2 17:32:39 2023 Construct fuzzy simplicial set\n",
      "Wed Aug  2 17:32:39 2023 Finding Nearest Neighbors\n",
      "Wed Aug  2 17:32:39 2023 Building RP forest with 21 trees\n",
      "Wed Aug  2 17:32:41 2023 NN descent for 17 iterations\n",
      "\t 1  /  17\n",
      "\t 2  /  17\n",
      "\t 3  /  17\n",
      "\t 4  /  17\n",
      "\t 5  /  17\n",
      "\t 6  /  17\n",
      "\t 7  /  17\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Wed Aug  2 17:32:44 2023 Finished Nearest Neighbor Search\n",
      "Wed Aug  2 17:32:44 2023 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [00:32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  2 17:33:25 2023 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction\n",
    "umap_model = umap.UMAP(n_neighbors=15, \n",
    "                       n_components=5, \n",
    "                       min_dist=0.0, \n",
    "                       metric='cosine', \n",
    "                       random_state=random_state, \n",
    "                       verbose=True)\n",
    "embedding = umap_model.fit_transform(passages_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = int(np.sqrt(len(embedding)))\n",
    "n_clusters = 350\n",
    "\n",
    "clustering_model = BisectingKMeans(n_clusters=n_clusters, \n",
    "                       n_init=1,\n",
    "                       bisecting_strategy='largest_cluster',\n",
    "                       random_state=random_state)\n",
    "topics = clustering_model.fit_predict(embedding)\n",
    "\n",
    "# clustering_model = GaussianMixture(n_components=n_clusters, \n",
    "#                        n_init=1,\n",
    "#                        init_params='k-means++',\n",
    "#                        covariance_type='diag',\n",
    "#                        reg_covar=1e-3,\n",
    "#                        random_state=random_state)\n",
    "# topics = clustering_model.fit_predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in conjunction with the virus this organism produced a vigorous leukocytic reaction.']\n",
      "[313]\n",
      "[0.00530461]\n",
      "[0.00271237]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "print(passages[50:51])\n",
    "print(clustering_model.predict(embedding[50:51]))\n",
    "t = clustering_model.transform(embedding[50:51])\n",
    "ts = softmax(np.exp(-t), axis=-1)\n",
    "print(ts[:,np.argmax(ts)])\n",
    "print(ts[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [03:03<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  2 17:17:40 2023 Worst tree score: 0.71285000\n",
      "Wed Aug  2 17:17:40 2023 Mean tree score: 0.71946571\n",
      "Wed Aug  2 17:17:40 2023 Best tree score: 0.72525000\n",
      "Wed Aug  2 17:17:42 2023 Forward diversification reduced edges from 1500000 to 610481\n",
      "Wed Aug  2 17:17:46 2023 Reverse diversification reduced edges from 610481 to 610481\n",
      "Wed Aug  2 17:17:48 2023 Degree pruning reduced edges from 774636 to 770039\n",
      "Wed Aug  2 17:17:48 2023 Resorting data and graph based on tree order\n",
      "Wed Aug  2 17:17:48 2023 Building and compiling search function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 30/30 [00:03]\n"
     ]
    }
   ],
   "source": [
    "other_p = model.encode(passages[100000:200000], show_progress_bar=True, batch_size=32)\n",
    "other_p = umap_model.transform(other_p)\n",
    "other_topics = clustering_model.predict(other_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_proportions(topics):\n",
    "    # first calculate base topic counts and proportions\n",
    "    outlier_count = 0\n",
    "    topic_counts = {topic:0 for topic in range(n_clusters)}\n",
    "    topic_proportions = {}\n",
    "    non_outlier_docs = 0\n",
    "    for idx, topic in enumerate(topics):\n",
    "        if topic == -1:\n",
    "            outlier_count += 1\n",
    "            continue\n",
    "        non_outlier_docs += 1\n",
    "        topic_counts[topic] += 1\n",
    "    for topic in range(n_clusters):\n",
    "        topic_proportions[topic] = topic_counts[topic] / non_outlier_docs\n",
    "    outlier_proportion = outlier_count / len(topics)\n",
    "    return topic_proportions, outlier_proportion\n",
    "\n",
    "\n",
    "def get_intersection(topic_proportions_a, topic_proportions_b):\n",
    "    total = 0\n",
    "    for topic in range(n_clusters):\n",
    "        total += min(topic_proportions_a[topic], topic_proportions_b[topic])\n",
    "    return total\n",
    "\n",
    "\n",
    "def calculate_topic_intersection(base_topics, other_topics):\n",
    "    base_topic_proportions, base_outlier_proportion = get_topic_proportions(base_topics)\n",
    "    other_topic_proportions, other_outlier_proportion = get_topic_proportions(other_topics)\n",
    "\n",
    "    # calc\n",
    "    intersection = get_intersection(base_topic_proportions, other_topic_proportions)\n",
    "    print(f\"base_outliers: {base_outlier_proportion:.2%}, other_outliers: {other_outlier_proportion:.2%}, intersection: {intersection:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_outliers: 0.00%, other_outliers: 0.00%, intersection: 72.60%\n"
     ]
    }
   ],
   "source": [
    "base_topic_proportions, base_outlier_proportion = get_topic_proportions(topics)\n",
    "other_topic_proportions, other_outlier_proportion = get_topic_proportions(other_topics)\n",
    "\n",
    "# calc\n",
    "intersection = get_intersection(base_topic_proportions, other_topic_proportions)\n",
    "print(f\"base_outliers: {base_outlier_proportion:.2%}, other_outliers: {other_outlier_proportion:.2%}, intersection: {intersection:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Topics\": [\"outliers\"] + [str(topic) for topic in range(n_clusters)],\n",
    "    'Base': [base_outlier_proportion]+[base_topic_proportions[topic] for topic in range(n_clusters)],\n",
    "    'Other': [other_outlier_proportion]+[other_topic_proportions[topic] for topic in range(n_clusters)],\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame = df,\n",
    "    x = \"Topics\",\n",
    "    y = [\"Base\",\"Other\"],\n",
    "    opacity = 0.9,\n",
    "    orientation = \"v\",\n",
    "    barmode = 'group',\n",
    "    title='Topic Proportions',\n",
    "    color_discrete_sequence=px.colors.qualitative.D3\n",
    "    #color_discrete_sequence=px.colors.sequential.Inferno_r\n",
    ")\n",
    "fig.update_yaxes(range=[0.0, 0.035])\n",
    "fig.write_html(\"trec_covid_small.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:00, 1279913.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    passages_contrast = []\n",
    "    passage_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        passages_contrast.append(passage_text_cleaned)\n",
    "        passage_uids.append(cord_uid)\n",
    "print(passages_contrast[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [00:24<00:00, 127.77it/s]\n",
      "Epochs completed: 100%| ██████████ 30/30 [00:05]\n",
      "2023-07-18 15:31:03,554 - BERTopic - Reduced dimensionality\n",
      "2023-07-18 15:31:03,631 - BERTopic - Predicted clusters\n"
     ]
    }
   ],
   "source": [
    "contrast_topics, _ = topic_model.transform(passages_contrast[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_outliers: 0.00%, other_outliers: 0.00%, intersection: 57.84%, MAE: 0.0024, AE: 0.8432\n"
     ]
    }
   ],
   "source": [
    "contrast_topic_proportions, contrast_outlier_proportion = get_topic_proportions(contrast_topics)\n",
    "\n",
    "# calc\n",
    "intersection = get_intersection(base_topic_proportions, contrast_topic_proportions)\n",
    "mae = get_MAE(base_topic_proportions, contrast_topic_proportions)\n",
    "ae = get_absolute_error(base_topic_proportions, contrast_topic_proportions)\n",
    "print(f\"base_outliers: {base_outlier_proportion:.2%}, other_outliers: {other_outlier_proportion:.2%}, intersection: {intersection:.2%}, MAE: {mae:.4f}, AE: {ae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Topics\": [\"outliers\"] + [str(topic) for topic in range(len(topic2desc))],\n",
    "    \"Topics_Names\": [\"outliers\"] + [topic2desc[topic] for topic in range(len(topic2desc))],\n",
    "    'Base': [base_outlier_proportion]+[base_topic_proportions[topic] for topic in range(len(topic2desc))],\n",
    "    'Other': [contrast_outlier_proportion]+[contrast_topic_proportions[topic] for topic in range(len(topic2desc))],\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame = df,\n",
    "    x = \"Topics\",\n",
    "    y = [\"Base\",\"Other\"],\n",
    "    opacity = 0.9,\n",
    "    orientation = \"v\",\n",
    "    barmode = 'group',\n",
    "    title='Topic Proportions',\n",
    "    hover_data=[\"Topics_Names\"],\n",
    "    color_discrete_sequence=px.colors.qualitative.D3\n",
    "    #color_discrete_sequence=px.colors.sequential.Inferno_r\n",
    ")\n",
    "fig.update_yaxes(range=[0.0, 0.035])\n",
    "fig.write_html(\"trec_covid_small_contrast.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

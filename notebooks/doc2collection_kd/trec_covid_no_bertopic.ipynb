{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "random_state = 42\n",
    "#random_state = 420\n",
    "random.seed(random_state)\n",
    "\n",
    "f_tokenized_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/0.csv\"\n",
    "f_tokenized_other_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/11.csv\"\n",
    "f_tokenized_contrast_path = \"/home/tfink/projects/rsa/kodicare/kodicare_framework/data/trec_covid_topic_modelling/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cleaned(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "        documents = {}\n",
    "        document_uids = []\n",
    "        for line in tqdm(reader):\n",
    "            cord_uid, passage_text_cleaned = line\n",
    "            if cord_uid not in documents:\n",
    "                documents[cord_uid] = []\n",
    "                document_uids.append(cord_uid)\n",
    "            documents[cord_uid].append(passage_text_cleaned)\n",
    "        return documents, document_uids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115954it [00:08, 129726.46it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glycyrrhiza glabra l]. glycyrrhiza glabra l].']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[document_uids[30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic TF-IDF delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29487, 619934)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc_iter(doc_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        yield \" \".join(doc_dict[uid])\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.75)\n",
    "X_train = count_vect.fit_transform(doc_iter(doc_dict=documents, document_uids=document_uids))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29487, 619934)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ks = [1,3,10]\n",
    "\n",
    "def batched_cosine_similarity(X,Y, batch_size=4096, dtype=np.float64):\n",
    "    X_size = X.shape[0]\n",
    "    Y_size = Y.shape[0]\n",
    "    steps = int(np.ceil(Y_size / batch_size))\n",
    "    sims = np.zeros((X_size, Y_size), dtype=dtype)\n",
    "    for i in tqdm(range(steps)):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        sim = cosine_similarity(X,Y[start_idx:end_idx,:]).astype(dtype=dtype)\n",
    "        sims[:,start_idx:end_idx] = sim\n",
    "    return sims\n",
    "\n",
    "def batched_cosine_similarity_topks(X,Y, batch_size=4096, dtype=np.float64, top_ks=[1,3,10]):\n",
    "    Y_size = Y.shape[0]\n",
    "    steps = int(np.ceil(Y_size / batch_size))\n",
    "    top_k_means = np.zeros((Y_size, len(top_ks)), dtype=dtype)\n",
    "    for i in tqdm(range(steps)):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        sim = cosine_similarity(X,Y[start_idx:end_idx,:]).astype(dtype=dtype)\n",
    "        for top_k_idx in range(len(top_ks)):\n",
    "            top_k = top_ks[top_k_idx]\n",
    "            mean_sims = get_mean_similarities(sim, top_k=top_k)\n",
    "            top_k_means[start_idx:end_idx,top_k_idx] = mean_sims\n",
    "    return np.mean(top_k_means, axis=0)\n",
    "\n",
    "\n",
    "def get_mean_similarities(sim_X_Y, top_k):\n",
    "    # https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "    Y_size = sim_X_Y.shape[1]\n",
    "    mean_sims = []\n",
    "    for i in range(Y_size):\n",
    "        #top_y_sims = sorted(sims[:,i], reverse=True)[:top_k]\n",
    "        top_y_sims_idx = np.argpartition(sim_X_Y[:,i], -top_k)[-top_k:]\n",
    "        top_y_sims = sim_X_Y[:,i][top_y_sims_idx]\n",
    "        mean_sims.append(np.mean(top_y_sims))\n",
    "    return np.array(mean_sims)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_means = batched_cosine_similarity_topks(X_train,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "top_1, top_3, top_10 = sim_means\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")\n",
    "sim_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1102137it [00:08, 130331.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29488, 619934)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_other_path)\n",
    "X_test = count_vect.transform(doc_iter(doc_dict=documents, document_uids=document_uids))\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:49<00:00, 21.16s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.4280, Top3: 0.3877, Top10: 0.3325\n"
     ]
    }
   ],
   "source": [
    "top_1, top_3, top_10 = sim_means\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast Dataset Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:00, 1278293.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    documents = []\n",
    "    document_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        documents.append(passage_text_cleaned)\n",
    "        document_uids.append(cord_uid)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 619934)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = count_vect.transform(documents)\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [39:28<00:00,  7.79s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.2435, Top3: 0.2009, Top10: 0.1461\n"
     ]
    }
   ],
   "source": [
    "top_1, top_3, top_10 = sim_means\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Vocab based delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_iter(doc_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        for passage in doc_dict[uid]:\n",
    "            yield passage, uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115954it [00:08, 135079.28it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_path)\n",
    "passages, passage_uids = zip(*list(passage_iter(doc_dict=documents, document_uids=document_uids[:10000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(embedding_model)\n",
    "passages_encoded = model.encode(passages, show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0, n_components=5, random_state=42, verbose=True)\n",
      "Tue Aug 22 17:02:53 2023 Construct fuzzy simplicial set\n",
      "Tue Aug 22 17:02:54 2023 Finding Nearest Neighbors\n",
      "Tue Aug 22 17:02:54 2023 Building RP forest with 32 trees\n",
      "Tue Aug 22 17:03:04 2023 NN descent for 18 iterations\n",
      "\t 1  /  18\n",
      "\t 2  /  18\n",
      "\t 3  /  18\n",
      "\t 4  /  18\n",
      "\t 5  /  18\n",
      "\t 6  /  18\n",
      "\t 7  /  18\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Tue Aug 22 17:03:30 2023 Finished Nearest Neighbor Search\n",
      "Tue Aug 22 17:03:34 2023 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [01:35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 22 17:05:58 2023 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction\n",
    "umap_model = umap.UMAP(n_neighbors=15, \n",
    "                       n_components=5, \n",
    "                       min_dist=0.0, \n",
    "                       metric='cosine', \n",
    "                       random_state=random_state, \n",
    "                       verbose=True)\n",
    "embedding = umap_model.fit_transform(passages_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "n_clusters = int(np.sqrt(len(embedding)))\n",
    "\n",
    "clustering_model = BisectingKMeans(n_clusters=n_clusters, \n",
    "                       n_init=1,\n",
    "                       bisecting_strategy='largest_cluster',\n",
    "                       random_state=random_state)\n",
    "topics = clustering_model.fit_predict(embedding)\n",
    "\n",
    "# clustering_model = GaussianMixture(n_components=n_clusters, \n",
    "#                        n_init=1,\n",
    "#                        init_params='k-means++',\n",
    "#                        covariance_type='diag',\n",
    "#                        reg_covar=1e-3,\n",
    "#                        random_state=random_state)\n",
    "# topics = clustering_model.fit_predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = {}\n",
    "\n",
    "for topic, passage_uid in zip(topics, passage_uids):\n",
    "    if passage_uid not in document_topics:\n",
    "        document_topics[passage_uid] = []\n",
    "    document_topics[passage_uid].append(str(topic))\n",
    "\n",
    "\n",
    "for document_uid in document_uids[:10000]:\n",
    "    document_topics[document_uid] = \" \".join(document_topics[document_uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'523'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[document_uids[30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc_topic_iter(doc_topic_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        yield doc_topic_dict[uid]\n",
    "\n",
    "\n",
    "# CountVectorizer needs custom token_pattern to include 1-character tokens (clusters 0-9)\n",
    "count_vect = CountVectorizer(max_df=1.0, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X_train = count_vect.fit_transform(doc_topic_iter(doc_topic_dict=document_topics, document_uids=document_uids[:10000]))\n",
    "assert count_vect.vocabulary_.get('0') != None\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 1.0000, Top3: 0.9224, Top10: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.9224, 0.851 ], dtype=float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks(X_train,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "top_1, top_3, top_10 = sim_means\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")\n",
    "sim_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13906/13906 [13:37<00:00, 17.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 22 17:23:55 2023 Worst tree score: 0.71120688\n",
      "Tue Aug 22 17:23:55 2023 Mean tree score: 0.71581295\n",
      "Tue Aug 22 17:23:55 2023 Best tree score: 0.71925507\n",
      "Tue Aug 22 17:23:58 2023 Forward diversification reduced edges from 4333275 to 1746936\n",
      "Tue Aug 22 17:24:02 2023 Reverse diversification reduced edges from 1746936 to 1746936\n",
      "Tue Aug 22 17:24:04 2023 Degree pruning reduced edges from 2231508 to 2215553\n",
      "Tue Aug 22 17:24:04 2023 Resorting data and graph based on tree order\n",
      "Tue Aug 22 17:24:04 2023 Building and compiling search function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 30/30 [00:15]\n"
     ]
    }
   ],
   "source": [
    "passages, passage_uids = zip(*list(passage_iter(doc_dict=documents, document_uids=document_uids[10000:20000])))\n",
    "other_p = model.encode(passages, show_progress_bar=True, batch_size=32)\n",
    "other_p = umap_model.transform(other_p)\n",
    "other_topics = clustering_model.predict(other_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics = {}\n",
    "\n",
    "for topic, passage_uid in zip(other_topics, passage_uids):\n",
    "    if passage_uid not in document_topics:\n",
    "        document_topics[passage_uid] = []\n",
    "    document_topics[passage_uid].append(str(topic))\n",
    "\n",
    "\n",
    "for document_uid in document_uids[10000:20000]:\n",
    "    document_topics[document_uid] = \" \".join(document_topics[document_uid])\n",
    "\n",
    "X_test = count_vect.transform(doc_topic_iter(doc_topic_dict=document_topics, document_uids=document_uids[10000:20000]))\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.8242, Top3: 0.8032, Top10: 0.7549\n"
     ]
    }
   ],
   "source": [
    "top_1, top_3, top_10 = sim_means\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast Dataset Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:01, 1230402.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    documents = []\n",
    "    document_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        documents.append(passage_text_cleaned)\n",
    "        document_uids.append(cord_uid)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [00:02<00:00, 119.92it/s]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:02]\n"
     ]
    }
   ],
   "source": [
    "other_p = model.encode(documents[:10000], show_progress_bar=True, batch_size=32)\n",
    "other_p = umap_model.transform(other_p)\n",
    "other_topics = clustering_model.predict(other_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_topics = [str(topic) for topic in other_topics]\n",
    "\n",
    "X_test = count_vect.transform(other_topics)\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.9883, Top3: 0.9707, Top10: 0.8813\n"
     ]
    }
   ],
   "source": [
    "top_1, top_3, top_10 = sim_means\n",
    "print(f\"Top1: {top_1:.4f}, Top3: {top_3:.4f}, Top10: {top_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

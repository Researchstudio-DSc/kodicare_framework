{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "random_state = 42\n",
    "#random_state = 420\n",
    "random.seed(random_state)\n",
    "\n",
    "f_tokenized_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/0.csv\"\n",
    "f_tokenized_other_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/11.csv\"\n",
    "f_tokenized_contrast_path = \"/home/tfink/projects/rsa/kodicare/kodicare_framework/data/trec_covid_topic_modelling/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cleaned(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "        documents = {}\n",
    "        document_uids = []\n",
    "        for line in tqdm(reader):\n",
    "            cord_uid, passage_text_cleaned = line\n",
    "            if cord_uid not in documents:\n",
    "                documents[cord_uid] = []\n",
    "                document_uids.append(cord_uid)\n",
    "            documents[cord_uid].append(passage_text_cleaned)\n",
    "        return documents, document_uids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115954it [00:08, 131887.62it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glycyrrhiza glabra l]. glycyrrhiza glabra l].']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[document_uids[30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic TF-IDF delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29487, 619934)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc_iter(doc_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        yield \" \".join(doc_dict[uid])\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.75)\n",
    "X_train = count_vect.fit_transform(doc_iter(doc_dict=documents, document_uids=document_uids))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29487, 619934)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ks = [1,10,100]\n",
    "\n",
    "def batched_cosine_similarity(X,Y, batch_size=4096, dtype=np.float64):\n",
    "    X_size = X.shape[0]\n",
    "    Y_size = Y.shape[0]\n",
    "    steps = int(np.ceil(Y_size / batch_size))\n",
    "    sims = np.zeros((X_size, Y_size), dtype=dtype)\n",
    "    for i in tqdm(range(steps)):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        sim = cosine_similarity(X,Y[start_idx:end_idx,:]).astype(dtype=dtype)\n",
    "        sims[:,start_idx:end_idx] = sim\n",
    "    return sims\n",
    "\n",
    "def batched_cosine_similarity_topks_Y2X(X,Y, batch_size=4096, dtype=np.float64, top_ks=[1,3,10]):\n",
    "    # compute cosine similiarities between each vector in Y (batched) with each vector in X\n",
    "    # for each vector y in Y, get the top k most similar vectors in X, and compute the mean over them\n",
    "    Y_size = Y.shape[0]\n",
    "    steps = int(np.ceil(Y_size / batch_size))\n",
    "    top_k_means = np.zeros((Y_size, len(top_ks)), dtype=dtype)\n",
    "    for i in tqdm(range(steps)):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        sim = cosine_similarity(X,Y[start_idx:end_idx,:]).astype(dtype=dtype)\n",
    "        for top_k_idx in range(len(top_ks)):\n",
    "            top_k = top_ks[top_k_idx]\n",
    "            mean_sims = get_mean_similarities(sim, top_k=top_k)\n",
    "            top_k_means[start_idx:end_idx,top_k_idx] = mean_sims\n",
    "    return np.mean(top_k_means, axis=0)\n",
    "\n",
    "\n",
    "def get_mean_similarities(sim_X_Y, top_k):\n",
    "    # https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "    Y_size = sim_X_Y.shape[1]\n",
    "    mean_sims = []\n",
    "    for i in range(Y_size):\n",
    "        #top_y_sims = sorted(sims[:,i], reverse=True)[:top_k]\n",
    "        top_y_sims_idx = np.argpartition(sim_X_Y[:,i], -top_k)[-top_k:]\n",
    "        top_y_sims = sim_X_Y[:,i][top_y_sims_idx]\n",
    "        mean_sims.append(np.mean(top_y_sims))\n",
    "    return np.array(mean_sims)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:49<00:00, 21.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 1.0000, Top10: 0.4514, Top100: 0.2683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.4514, 0.2683], dtype=float16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks_Y2X(X_train,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "print(f\"Top{top_ks[0]}: {sim_means[0]:.4f}, Top{top_ks[1]}: {sim_means[1]:.4f}, Top{top_ks[2]}: {sim_means[2]:.4f}\")\n",
    "sim_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1102137it [00:08, 126462.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29488, 619934)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_other_path)\n",
    "X_test = count_vect.transform(doc_iter(doc_dict=documents, document_uids=document_uids))\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:45<00:00, 20.67s/it]\n",
      "100%|██████████| 8/8 [02:44<00:00, 20.52s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means_test2train = batched_cosine_similarity_topks_Y2X(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "sim_means_train2test = batched_cosine_similarity_topks_Y2X(X_test,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test2Train Top1: 0.4280, Top10: 0.3325, Top100: 0.2223\n",
      "Train2Test Top1: 0.4336, Top10: 0.3408, Top100: 0.2312\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test2Train Top{top_ks[0]}: {sim_means_test2train[0]:.4f}, Top{top_ks[1]}: {sim_means_test2train[1]:.4f}, Top{top_ks[2]}: {sim_means_test2train[2]:.4f}\")\n",
    "print(f\"Train2Test Top{top_ks[0]}: {sim_means_train2test[0]:.4f}, Top{top_ks[1]}: {sim_means_train2test[1]:.4f}, Top{top_ks[2]}: {sim_means_train2test[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast Dataset Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:01, 1191319.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    documents = []\n",
    "    document_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        documents.append(passage_text_cleaned)\n",
    "        document_uids.append(cord_uid)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 619934)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = count_vect.transform(documents)\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [40:11<00:00,  7.93s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means_test2train = batched_cosine_similarity_topks_Y2X(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "sim_means_train2test = batched_cosine_similarity_topks_Y2X(X_test,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 0.2435, Top10: 0.1461, Top100: 0.0609\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test2Train Top{top_ks[0]}: {sim_means_test2train[0]:.4f}, Top{top_ks[1]}: {sim_means_test2train[1]:.4f}, Top{top_ks[2]}: {sim_means_test2train[2]:.4f}\")\n",
    "print(f\"Train2Test Top{top_ks[0]}: {sim_means_train2test[0]:.4f}, Top{top_ks[1]}: {sim_means_train2test[1]:.4f}, Top{top_ks[2]}: {sim_means_train2test[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Vocab based delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_iter(doc_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        for passage in doc_dict[uid]:\n",
    "            yield passage, uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115954it [00:08, 128269.29it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, document_uids = read_cleaned(f_tokenized_path)\n",
    "passages, passage_uids = zip(*list(passage_iter(doc_dict=documents, document_uids=document_uids[:10000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9028/9028 [08:43<00:00, 17.24it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(embedding_model)\n",
    "passages_encoded = model.encode(passages, show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0, n_components=5, random_state=42, verbose=True)\n",
      "Wed Aug 23 12:22:07 2023 Construct fuzzy simplicial set\n",
      "Wed Aug 23 12:22:07 2023 Finding Nearest Neighbors\n",
      "Wed Aug 23 12:22:07 2023 Building RP forest with 32 trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 23 12:22:18 2023 NN descent for 18 iterations\n",
      "\t 1  /  18\n",
      "\t 2  /  18\n",
      "\t 3  /  18\n",
      "\t 4  /  18\n",
      "\t 5  /  18\n",
      "\t 6  /  18\n",
      "\t 7  /  18\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Wed Aug 23 12:22:42 2023 Finished Nearest Neighbor Search\n",
      "Wed Aug 23 12:22:45 2023 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [01:32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 23 12:25:01 2023 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction\n",
    "umap_model = umap.UMAP(n_neighbors=15, \n",
    "                       n_components=5, \n",
    "                       min_dist=0.0, \n",
    "                       metric='cosine', \n",
    "                       random_state=random_state, \n",
    "                       verbose=True)\n",
    "embedding = umap_model.fit_transform(passages_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "n_clusters = int(np.sqrt(len(embedding)))\n",
    "\n",
    "clustering_model = BisectingKMeans(n_clusters=n_clusters, \n",
    "                       n_init=1,\n",
    "                       bisecting_strategy='largest_cluster',\n",
    "                       random_state=random_state)\n",
    "topics = clustering_model.fit_predict(embedding)\n",
    "\n",
    "# clustering_model = GaussianMixture(n_components=n_clusters, \n",
    "#                        n_init=1,\n",
    "#                        init_params='k-means++',\n",
    "#                        covariance_type='diag',\n",
    "#                        reg_covar=1e-3,\n",
    "#                        random_state=random_state)\n",
    "# topics = clustering_model.fit_predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = {}\n",
    "\n",
    "for topic, passage_uid in zip(topics, passage_uids):\n",
    "    if passage_uid not in document_topics:\n",
    "        document_topics[passage_uid] = []\n",
    "    document_topics[passage_uid].append(str(topic))\n",
    "\n",
    "\n",
    "for document_uid in document_uids[:10000]:\n",
    "    document_topics[document_uid] = \" \".join(document_topics[document_uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'523'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[document_uids[30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc_topic_iter(doc_topic_dict, document_uids):\n",
    "    for uid in document_uids:\n",
    "        yield doc_topic_dict[uid]\n",
    "\n",
    "\n",
    "# CountVectorizer needs custom token_pattern to include 1-character tokens (clusters 0-9)\n",
    "count_vect = CountVectorizer(max_df=1.0, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X_train = count_vect.fit_transform(doc_topic_iter(doc_topic_dict=document_topics, document_uids=document_uids[:10000]))\n",
    "assert count_vect.vocabulary_.get('0') != None\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1: 1.0000, Top10: 0.8511, Top100: 0.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.851 , 0.4995], dtype=float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_means = batched_cosine_similarity_topks_Y2X(X_train,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "print(f\"Top{top_ks[0]}: {sim_means[0]:.4f}, Top{top_ks[1]}: {sim_means[1]:.4f}, Top{top_ks[2]}: {sim_means[2]:.4f}\")\n",
    "sim_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   1%|          | 104/13906 [00:06<13:37, 16.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13906/13906 [13:30<00:00, 17.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 23 12:38:59 2023 Worst tree score: 0.71120688\n",
      "Wed Aug 23 12:38:59 2023 Mean tree score: 0.71581295\n",
      "Wed Aug 23 12:38:59 2023 Best tree score: 0.71925507\n",
      "Wed Aug 23 12:39:02 2023 Forward diversification reduced edges from 4333275 to 1746936\n",
      "Wed Aug 23 12:39:05 2023 Reverse diversification reduced edges from 1746936 to 1746936\n",
      "Wed Aug 23 12:39:07 2023 Degree pruning reduced edges from 2231508 to 2215553\n",
      "Wed Aug 23 12:39:07 2023 Resorting data and graph based on tree order\n",
      "Wed Aug 23 12:39:07 2023 Building and compiling search function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 30/30 [00:13]\n"
     ]
    }
   ],
   "source": [
    "passages, passage_uids = zip(*list(passage_iter(doc_dict=documents, document_uids=document_uids[10000:20000])))\n",
    "other_p = model.encode(passages, show_progress_bar=True, batch_size=32)\n",
    "other_p = umap_model.transform(other_p)\n",
    "other_topics = clustering_model.predict(other_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics = {}\n",
    "\n",
    "for topic, passage_uid in zip(other_topics, passage_uids):\n",
    "    if passage_uid not in document_topics:\n",
    "        document_topics[passage_uid] = []\n",
    "    document_topics[passage_uid].append(str(topic))\n",
    "\n",
    "\n",
    "for document_uid in document_uids[10000:20000]:\n",
    "    document_topics[document_uid] = \" \".join(document_topics[document_uid])\n",
    "\n",
    "X_test = count_vect.transform(doc_topic_iter(doc_topic_dict=document_topics, document_uids=document_uids[10000:20000]))\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means_test2train = batched_cosine_similarity_topks_Y2X(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "sim_means_train2test = batched_cosine_similarity_topks_Y2X(X_test,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test2Train Top1: 0.8242, Top10: 0.7549, Top100: 0.4419\n",
      "Train2Test Top1: 0.8662, Top10: 0.7666, Top100: 0.4436\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test2Train Top{top_ks[0]}: {sim_means_test2train[0]:.4f}, Top{top_ks[1]}: {sim_means_test2train[1]:.4f}, Top{top_ks[2]}: {sim_means_test2train[2]:.4f}\")\n",
    "print(f\"Train2Test Top{top_ks[0]}: {sim_means_train2test[0]:.4f}, Top{top_ks[1]}: {sim_means_train2test[1]:.4f}, Top{top_ks[2]}: {sim_means_train2test[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast Dataset Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:01, 1242874.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    documents = []\n",
    "    document_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        documents.append(passage_text_cleaned)\n",
    "        document_uids.append(cord_uid)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [00:02<00:00, 124.22it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:02]\n"
     ]
    }
   ],
   "source": [
    "other_p = model.encode(documents[:10000], show_progress_bar=True, batch_size=32)\n",
    "other_p = umap_model.transform(other_p)\n",
    "other_topics = clustering_model.predict(other_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 537)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_topics = [str(topic) for topic in other_topics]\n",
    "\n",
    "X_test = count_vect.transform(other_topics)\n",
    "X_test = tf_transformer.transform(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.63s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_means_test2train = batched_cosine_similarity_topks_Y2X(X_train,X_test, batch_size=4096, dtype=np.float16, top_ks=top_ks)\n",
    "sim_means_train2test = batched_cosine_similarity_topks_Y2X(X_test,X_train, batch_size=4096, dtype=np.float16, top_ks=top_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test2Train Top1: 0.9883, Top10: 0.8813, Top100: 0.3257\n",
      "Train2Test Top1: 0.8379, Top10: 0.6958, Top100: 0.2568\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test2Train Top{top_ks[0]}: {sim_means_test2train[0]:.4f}, Top{top_ks[1]}: {sim_means_test2train[1]:.4f}, Top{top_ks[2]}: {sim_means_test2train[2]:.4f}\")\n",
    "print(f\"Train2Test Top{top_ks[0]}: {sim_means_train2test[0]:.4f}, Top{top_ks[1]}: {sim_means_train2test[1]:.4f}, Top{top_ks[2]}: {sim_means_train2test[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

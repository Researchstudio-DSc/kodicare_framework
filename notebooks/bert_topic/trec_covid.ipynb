{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tfink/miniconda3/envs/bertopic/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.cluster import KMeans, BisectingKMeans\n",
    "random_state = 42\n",
    "#random_state = 420\n",
    "random.seed(random_state)\n",
    "\n",
    "f_tokenized_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/0.csv\"\n",
    "f_tokenized_other_path = \"/home/tfink/data/kodicare/trec-covid/dtc_evolving_bert/11.csv\"\n",
    "f_tokenized_contrast_path = \"/home/tfink/projects/rsa/kodicare/kodicare_framework/data/trec_covid_topic_modelling/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cleaned(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "        passages = []\n",
    "        passage_uids = []\n",
    "        for line in tqdm(reader):\n",
    "            cord_uid, passage_text_cleaned = line\n",
    "            passages.append(passage_text_cleaned)\n",
    "            passage_uids.append(cord_uid)\n",
    "        return passages, passage_uids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115954it [00:08, 131104.60it/s]\n"
     ]
    }
   ],
   "source": [
    "passages, passage_uids = read_cleaned(f_tokenized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('in conjunction with the virus this organism produced a vigorous leukocytic reaction.',\n",
       " '04ceiyko')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages[50], passage_uids[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [03:01<00:00, 17.21it/s]\n",
      "2023-07-18 15:21:51,770 - BERTopic - Transformed documents to Embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0, n_components=5, random_state=42, verbose=True)\n",
      "Tue Jul 18 15:21:51 2023 Construct fuzzy simplicial set\n",
      "Tue Jul 18 15:21:51 2023 Finding Nearest Neighbors\n",
      "Tue Jul 18 15:21:51 2023 Building RP forest with 21 trees\n",
      "Tue Jul 18 15:21:56 2023 NN descent for 17 iterations\n",
      "\t 1  /  17\n",
      "\t 2  /  17\n",
      "\t 3  /  17\n",
      "\t 4  /  17\n",
      "\t 5  /  17\n",
      "\t 6  /  17\n",
      "\t 7  /  17\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Tue Jul 18 15:22:17 2023 Finished Nearest Neighbor Search\n",
      "Tue Jul 18 15:22:20 2023 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [00:34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 18 15:23:03 2023 Finished embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 15:23:03,485 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 15:23:04,208 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "min_topic_size = 10\n",
    "min_samples = None\n",
    "#cluster_selection_epsilon = 0.25\n",
    "cluster_selection_epsilon = 0.15\n",
    "# dimensionality reduction\n",
    "umap_model = umap.UMAP(n_neighbors=15, \n",
    "                       n_components=5, \n",
    "                       min_dist=0.0, \n",
    "                       metric='cosine', \n",
    "                       random_state=random_state, \n",
    "                       verbose=True)\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_topic_size,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True,\n",
    "                        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "                        min_samples=min_samples)\n",
    "n_clusters = np.sqrt(len(passages))\n",
    "n_clusters = 350\n",
    "hdbscan_model = BisectingKMeans(n_clusters=n_clusters, \n",
    "                       n_init=1,\n",
    "                       bisecting_strategy='largest_cluster',\n",
    "                       random_state=random_state)\n",
    "# hdbscan_model = KMeans(n_clusters=n_clusters, \n",
    "#                        init='k-means++', \n",
    "#                        n_init='auto',\n",
    "#                        random_state=random_state)\n",
    "#hdbscan_model=None\n",
    "\n",
    "# vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# create BerTopic model\n",
    "#embedding_model = \"xlm-r-bert-base-nli-stsb-mean-tokens\"\n",
    "#embedding_model = \"all-mpnet-base-v2\"\n",
    "embedding_model = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "topic_model = BERTopic(embedding_model=embedding_model, \n",
    "                       umap_model=umap_model, \n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       vectorizer_model=vectorizer,\n",
    "                       language=\"english\", \n",
    "                       calculate_probabilities=False, \n",
    "                       min_topic_size=min_topic_size,\n",
    "                       verbose=True)\n",
    "\n",
    "# Perform topic modeling with BERTopic\n",
    "topics, probabilities = topic_model.fit_transform(passages[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>499</td>\n",
       "      <td>0_influenza_sialic_hemagglutinin_erythrocytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>1_flavivirus_viruses_genera_flaviviruses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>493</td>\n",
       "      <td>2_pulmonary_lung_ventilation_inhalation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>491</td>\n",
       "      <td>3_coronaviruses_coronavirus_229e_oc43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>483</td>\n",
       "      <td>4_liver_hepatic_aaf_epoxide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>20</td>\n",
       "      <td>345_server_file_embl_nucleotide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>346</td>\n",
       "      <td>20</td>\n",
       "      <td>346_turner_decoction_peoples_sore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>347</td>\n",
       "      <td>18</td>\n",
       "      <td>347_dhori_pbl_segment_influenza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>348</td>\n",
       "      <td>17</td>\n",
       "      <td>348_thalidomide_gvhd_teratogenicity_immunosupp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>17</td>\n",
       "      <td>349_tmvcp_polio_tmv_ptmvcp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name\n",
       "0        0    499      0_influenza_sialic_hemagglutinin_erythrocytes\n",
       "1        1    496           1_flavivirus_viruses_genera_flaviviruses\n",
       "2        2    493            2_pulmonary_lung_ventilation_inhalation\n",
       "3        3    491              3_coronaviruses_coronavirus_229e_oc43\n",
       "4        4    483                        4_liver_hepatic_aaf_epoxide\n",
       "..     ...    ...                                                ...\n",
       "345    345     20                    345_server_file_embl_nucleotide\n",
       "346    346     20                  346_turner_decoction_peoples_sore\n",
       "347    347     18                    347_dhori_pbl_segment_influenza\n",
       "348    348     17  348_thalidomide_gvhd_teratogenicity_immunosupp...\n",
       "349    349     17                         349_tmvcp_polio_tmv_ptmvcp\n",
       "\n",
       "[350 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2desc = {}\n",
    "for index, row in topic_info.iterrows():\n",
    "    if row.Topic == -1:\n",
    "        continue\n",
    "    topic2desc[row.Topic] = row.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [03:00<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 18 15:26:24 2023 Worst tree score: 0.71285000\n",
      "Tue Jul 18 15:26:24 2023 Mean tree score: 0.71946571\n",
      "Tue Jul 18 15:26:24 2023 Best tree score: 0.72525000\n",
      "Tue Jul 18 15:26:26 2023 Forward diversification reduced edges from 1500000 to 610481\n",
      "Tue Jul 18 15:26:29 2023 Reverse diversification reduced edges from 610481 to 610481\n",
      "Tue Jul 18 15:26:31 2023 Degree pruning reduced edges from 774636 to 770039\n",
      "Tue Jul 18 15:26:31 2023 Resorting data and graph based on tree order\n",
      "Tue Jul 18 15:26:31 2023 Building and compiling search function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 30/30 [00:03]\n",
      "2023-07-18 15:28:32,895 - BERTopic - Reduced dimensionality\n",
      "2023-07-18 15:28:33,002 - BERTopic - Predicted clusters\n"
     ]
    }
   ],
   "source": [
    "other_topics, _ = topic_model.transform(passages[100000:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_proportions(topics):\n",
    "    # first calculate base topic counts and proportions\n",
    "    outlier_count = 0\n",
    "    topic_counts = {topic:0 for topic in range(len(topic2desc))}\n",
    "    topic_proportions = {}\n",
    "    non_outlier_docs = 0\n",
    "    for idx, topic in enumerate(topics):\n",
    "        if topic == -1:\n",
    "            outlier_count += 1\n",
    "            continue\n",
    "        non_outlier_docs += 1\n",
    "        topic_counts[topic] += 1\n",
    "    for topic in range(len(topic2desc)):\n",
    "        topic_proportions[topic] = topic_counts[topic] / non_outlier_docs\n",
    "    outlier_proportion = outlier_count / len(topics)\n",
    "    return topic_proportions, outlier_proportion\n",
    "\n",
    "\n",
    "def get_intersection(topic_proportions_a, topic_proportions_b):\n",
    "    total = 0\n",
    "    for topic_desc in topic_proportions_a.keys():\n",
    "        total += min(topic_proportions_a[topic_desc], topic_proportions_b[topic_desc])\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_MAE(topic_proportions_a, topic_proportions_b):\n",
    "    total = 0\n",
    "    for topic_desc in topic_proportions_a.keys():\n",
    "        total += abs(topic_proportions_a[topic_desc] - topic_proportions_b[topic_desc])\n",
    "    return total / len(topic_proportions_a)\n",
    "\n",
    "\n",
    "def get_absolute_error(topic_proportions_a, topic_proportions_b):\n",
    "    total = 0\n",
    "    for topic_desc in topic_proportions_a.keys():\n",
    "        total += abs(topic_proportions_a[topic_desc] - topic_proportions_b[topic_desc])\n",
    "    return total\n",
    "\n",
    "\n",
    "def calculate_topic_intersection(base_topics, other_topics):\n",
    "    base_topic_proportions, base_outlier_proportion = get_topic_proportions(base_topics)\n",
    "    other_topic_proportions, other_outlier_proportion = get_topic_proportions(other_topics)\n",
    "\n",
    "    # calc\n",
    "    intersection = get_intersection(base_topic_proportions, other_topic_proportions)\n",
    "    print(f\"base_outliers: {base_outlier_proportion:.2%}, other_outliers: {other_outlier_proportion:.2%}, intersection: {intersection:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_outliers: 0.00%, other_outliers: 0.00%, intersection: 74.06%, MAE: 0.0015, AE: 0.5188\n"
     ]
    }
   ],
   "source": [
    "base_topic_proportions, base_outlier_proportion = get_topic_proportions(topics)\n",
    "other_topic_proportions, other_outlier_proportion = get_topic_proportions(other_topics)\n",
    "\n",
    "# calc\n",
    "intersection = get_intersection(base_topic_proportions, other_topic_proportions)\n",
    "mae = get_MAE(base_topic_proportions, other_topic_proportions)\n",
    "ae = get_absolute_error(base_topic_proportions, other_topic_proportions)\n",
    "print(f\"base_outliers: {base_outlier_proportion:.2%}, other_outliers: {other_outlier_proportion:.2%}, intersection: {intersection:.2%}, MAE: {mae:.4f}, AE: {ae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Topics\": [\"outliers\"] + [str(topic) for topic in range(len(topic2desc))],\n",
    "    \"Topics_Names\": [\"outliers\"] + [topic2desc[topic] for topic in range(len(topic2desc))],\n",
    "    'Base': [base_outlier_proportion]+[base_topic_proportions[topic] for topic in range(len(topic2desc))],\n",
    "    'Other': [other_outlier_proportion]+[other_topic_proportions[topic] for topic in range(len(topic2desc))],\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame = df,\n",
    "    x = \"Topics\",\n",
    "    y = [\"Base\",\"Other\"],\n",
    "    opacity = 0.9,\n",
    "    orientation = \"v\",\n",
    "    barmode = 'group',\n",
    "    title='Topic Proportions',\n",
    "    hover_data=[\"Topics_Names\"],\n",
    "    color_discrete_sequence=px.colors.qualitative.D3\n",
    "    #color_discrete_sequence=px.colors.sequential.Inferno_r\n",
    ")\n",
    "fig.update_yaxes(range=[0.0, 0.035])\n",
    "fig.write_html(\"trec_covid_small.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244184it [00:00, 1279913.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_tokenized_contrast_path, \"r\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    passages_contrast = []\n",
    "    passage_uids = []\n",
    "    # skip first line\n",
    "    reader.__next__()\n",
    "    for line in tqdm(reader):\n",
    "        cord_uid, passage_text_cleaned = line\n",
    "        passages_contrast.append(passage_text_cleaned)\n",
    "        passage_uids.append(cord_uid)\n",
    "print(passages_contrast[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [00:24<00:00, 127.77it/s]\n",
      "Epochs completed: 100%| ██████████ 30/30 [00:05]\n",
      "2023-07-18 15:31:03,554 - BERTopic - Reduced dimensionality\n",
      "2023-07-18 15:31:03,631 - BERTopic - Predicted clusters\n"
     ]
    }
   ],
   "source": [
    "contrast_topics, _ = topic_model.transform(passages_contrast[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_outliers: 0.00%, other_outliers: 0.00%, intersection: 57.84%, MAE: 0.0024, AE: 0.8432\n"
     ]
    }
   ],
   "source": [
    "contrast_topic_proportions, contrast_outlier_proportion = get_topic_proportions(contrast_topics)\n",
    "\n",
    "# calc\n",
    "intersection = get_intersection(base_topic_proportions, contrast_topic_proportions)\n",
    "mae = get_MAE(base_topic_proportions, contrast_topic_proportions)\n",
    "ae = get_absolute_error(base_topic_proportions, contrast_topic_proportions)\n",
    "print(f\"base_outliers: {base_outlier_proportion:.2%}, other_outliers: {other_outlier_proportion:.2%}, intersection: {intersection:.2%}, MAE: {mae:.4f}, AE: {ae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Topics\": [\"outliers\"] + [str(topic) for topic in range(len(topic2desc))],\n",
    "    \"Topics_Names\": [\"outliers\"] + [topic2desc[topic] for topic in range(len(topic2desc))],\n",
    "    'Base': [base_outlier_proportion]+[base_topic_proportions[topic] for topic in range(len(topic2desc))],\n",
    "    'Other': [contrast_outlier_proportion]+[contrast_topic_proportions[topic] for topic in range(len(topic2desc))],\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame = df,\n",
    "    x = \"Topics\",\n",
    "    y = [\"Base\",\"Other\"],\n",
    "    opacity = 0.9,\n",
    "    orientation = \"v\",\n",
    "    barmode = 'group',\n",
    "    title='Topic Proportions',\n",
    "    hover_data=[\"Topics_Names\"],\n",
    "    color_discrete_sequence=px.colors.qualitative.D3\n",
    "    #color_discrete_sequence=px.colors.sequential.Inferno_r\n",
    ")\n",
    "fig.update_yaxes(range=[0.0, 0.035])\n",
    "fig.write_html(\"trec_covid_small_contrast.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
